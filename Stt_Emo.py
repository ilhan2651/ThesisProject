import torch
import soundfile as sf
import numpy as np
import sounddevice as sd
import time
import speech_recognition as sr
import keyboard
import librosa
from transformers import Wav2Vec2FeatureExtractor, Wav2Vec2ForSequenceClassification

# ğŸš€ **GPU KullanÄ±mÄ±**
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"ğŸš€ Using device: {device}")

# ğŸ­ **Duygu Analizi Modeli**
emotion_model_name = "r-f/wav2vec-english-speech-emotion-recognition"
feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(emotion_model_name)
emotion_model = Wav2Vec2ForSequenceClassification.from_pretrained(emotion_model_name).to(device)

# ğŸ¤ **Mikrofon ile Ses KaydÄ± (Sadece 'q' tuÅŸu ile durur)**
def record_audio(filename="speech.wav"):
    fs = 16000
    channels = 1
    chunk_size = 1024
    buffer = []
    recording_active = True

    print("ğŸ¤ KonuÅŸmaya baÅŸlayÄ±n ('q' tuÅŸuna basarak kaydÄ± durdurabilirsiniz)...")

    def callback(indata, frames, time_info, status):
        nonlocal recording_active
        if keyboard.is_pressed("q"):
            print("ğŸ›‘ 'q' tuÅŸuna basÄ±ldÄ±. KayÄ±t durduruluyor...")
            recording_active = False
            raise sd.CallbackStop
        buffer.append(indata.copy())

    with sd.InputStream(samplerate=fs, channels=channels, callback=callback, blocksize=chunk_size):
        while recording_active:
            time.sleep(0.1)

    audio_data = np.concatenate(buffer, axis=0)
    sf.write(filename, audio_data, fs)

    return filename

# ğŸ™ï¸ **Google Speech API ile Metne Ã‡evirme**
def transcribe_audio(filename):
    recognizer = sr.Recognizer()
    with sr.AudioFile(filename) as source:
        print("ğŸ“ Metne Ã§eviriliyor...")
        audio = recognizer.record(source)
    try:
        text = recognizer.recognize_google(audio, language="tr-TR")  
        text = text.capitalize()
        if not text.endswith((".", "?", "!")):
            text += "."
        print(f"ğŸ“„ Metin Ã‡Ä±ktÄ±sÄ±: {text}")
        return text
    except sr.UnknownValueError:
        print("âŒ Google Speech Recognition sesi anlayamadÄ±.")
        return None
    except sr.RequestError:
        print("âŒ Google API'ye baÄŸlanÄ±lamadÄ±.")
        return None

# ğŸ­ **Duygu Analizi (GÃ¼ncellenmiÅŸ Algoritma)**
def predict_emotion(filename):
    print("ğŸ” Duygu analizi yapÄ±lÄ±yor...")

    # **Ses dosyasÄ±nÄ± yÃ¼kle**
    audio, sr = librosa.load(filename, sr=16000)

    # **Hugging Face modeline uygun formatta giriÅŸ verisi hazÄ±rla**
    inputs = feature_extractor(audio, sampling_rate=sr, return_tensors="pt", padding=True)
    inputs = inputs.to(device)

    # âœ… **Model ile tahmin yap**
    with torch.no_grad():
        outputs = emotion_model(inputs.input_values)
        predictions = torch.nn.functional.softmax(outputs.logits.mean(dim=1), dim=-1)  # Ortalama alarak stabil tahmin yap

    # âœ… **En yÃ¼ksek olasÄ±lÄ±ÄŸa sahip duyguyu bul**
    predicted_label = torch.argmax(predictions, dim=-1).item()
    emotion = emotion_model.config.id2label[predicted_label]

    # âœ… **Duygu yÃ¼zdelerini gÃ¶ster**
    emotion_scores = {emotion_model.config.id2label[i]: round(predictions[0][i].item() * 100, 2) for i in range(len(predictions[0]))}
    sorted_emotions = sorted(emotion_scores.items(), key=lambda x: x[1], reverse=True)

    print("\nğŸ­ **Duygu OlasÄ±lÄ±klarÄ±:**")
    for emo, score in sorted_emotions:
        print(f"   - {emo}: %{score}")

    return emotion

# ğŸ¯ **Ses Kaydet + Metne Ã‡evir + Duygu Analizi Yap**
if __name__ == "__main__":
    audio_file = record_audio()
    text = transcribe_audio(audio_file)  
    predict_emotion(audio_file)
